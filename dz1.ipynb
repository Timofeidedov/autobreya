{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "сначала займемся парсингом"
      ],
      "metadata": {
        "id": "AZAXMjKX1DPA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qw6_XxQI005U"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fake_useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3T9uazA1B3I",
        "outputId": "e57a8166-9f61-4a8b-ab2d-a3fec26c07f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fake_useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=449c93e178aff3565cfef9247dacb0da77101c5f93f0ce0d79432f91d17ebbf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = requests.session()"
      ],
      "metadata": {
        "id": "koaYutep1LHF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fake_useragent import UserAgent\n",
        "ua = UserAgent(verify_ssl=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svGVIIXF22q3",
        "outputId": "5d1c7c4e-535f-4496-f790-d1fb60cfe43b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fake_useragent:Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 154, in load\n",
            "    for item in get_browsers(verify_ssl=verify_ssl):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
            "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re "
      ],
      "metadata": {
        "id": "s34WmU5i6tej"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.debugger import bdb\n",
        "def get_nth_page(i,bg,bb):\n",
        "    # скачиваем\n",
        "    url = f'https://www.kinopoisk.ru/reviews/type/comment/period/month/page/{i}/#list'\n",
        "    request = requests.get(url)\n",
        "    if request.status_code == 200: \n",
        "      req = session.get(url, headers={'User-Agent': ua.random})\n",
        "      page = req.text\n",
        "      soup = BeautifulSoup(page, 'html.parser')\n",
        "      good=soup.find_all('div',attrs={'class':'response good'})\n",
        "      for smth in good: \n",
        "        good_words=smth.find_all('p')\n",
        "        for smth in good_words:\n",
        "          g=smth.text\n",
        "          g=g.replace('\\xa0',' ')\n",
        "          g=g.replace('\\n\\r\\n',' ')\n",
        "          bg.append(g)\n",
        "      bad=soup.find_all('div',attrs={'class':'response bad'})\n",
        "      for smth in bad: \n",
        "        bad_words=smth.find_all('p')\n",
        "        for smth in bad_words:\n",
        "          b=smth.text\n",
        "          b=b.replace('\\xa0',' ')\n",
        "          b=b.replace('\\n\\r\\n',' ')\n",
        "          bb.append(b)"
      ],
      "metadata": {
        "id": "P398uUbc2-nm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_good=[]\n",
        "bag_bad=[]"
      ],
      "metadata": {
        "id": "WcW2SFrK9orE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,31):\n",
        "  get_nth_page(i,bag_good,bag_bad)"
      ],
      "metadata": {
        "id": "Z1NapPGt1MOL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(bag_good))\n",
        "print(len(bag_bad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csFDm9JB90v4",
        "outputId": "47d9aa0a-9bd5-462b-8ce5-bca893c294d0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "953\n",
            "254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bag_bad[18])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjUAWCboGSvB",
        "outputId": "2cf88995-5050-45ff-aa3b-221a93d7b70d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Блондинка» Эндрю Доминика оказалась настолько ущербной картиной, что я впервые сел писать рецензию, не досмотрев фильм. Только вдумайтесь — ради полноценного отзыва я досмотрел такое убожество, как «Крым», я переборол свое желание свалить с киносеанса четвертой «Матрицы», я закатывал глаза, но продолжал смотреть «Эйфорию», и чтобы написать рецензию к «Необратимости», мне пришлось стиснуть зубы и терпеть. Вот тут не надо быть по образованию проктологом, чтобы понять, что с «Блондинкой» ситуация полная ж… есть. Не помню контекст, но кто-то из моих знакомых говорил, что ругать фильм гораздо тяжелее, чем хвалить. И насколько же мне трудно говорить про «Блондинку». Фильм получился плох не за какие-то ужасные отдельные элементы, а потому что он представляет сотканное белыми нитками нечто, что должно было позиционировать собой историю культовой американской актрисы и секс-символа Мэрилин Монро. Не буду врать — я ждал этот фильм. Новости о том, что фильм расскажет историю Мэрилин Монро, что главную роль сыграет Ана «Куба — любовь моя» де Армас и что кинолента удостоилась 14 минут оваций на Венецианском кинофестивале, само собой, подогревали мой интерес. Как говорится, а чего вы ждали от фильма про Мэрилин Монро? Но с момента, как кино стало доступно для широких масс на Netflix, оценки, наравне с улыбкой Нормы Джин Бейкер, только падают и падают. Зрители жалуются на бессвязность повествования, на подтасовку фактов и на неприятные сцены сексуального характера. Как говорится, а чего вы ждали от фильма про Мэрилин Монро? Явно не этого. Первый минус, который мне мозолил глаза — это курс режиссера/сценариста Эндрю Доминика в этом фильме. Я идентифицирую себя как человека, который с биографией Мэрилин Монро знаком на уровне Википедии и всяких тонкостей её души знать не могу. Поэтому я ждал от этого фильма большего раскрытия её истории. Беда в том, что фильм «Блондинка» основан на книге писательницы Джойс Кэрол Уотс, которая не является исторически достоверным произведением, а представлена попыткой посмотреть на мир вокруг Монро через её призму. То есть, это своеобразный фанфик. Фанфики грешны тем, что зачастую 14-летние девочки пишут истории про своих любимых персонажей и вертят ими, как хотят. Художественной ценности в этом нет. Отсюда и теряется интерес к показанной на экране истории. Причем фильм идет 2 часа 47 минут, но кажется двадцатичасовой пыткой, боль от которой сильнее, чем если засунуть пенис в глорихол с бритвенными лезвиями. Диалоги в фильме бессвязны, прочие персонажи, кроме героини Аны де Армас, безлики, отчего трудно понять, о чем они говорят. Плюс у кино нет временной шкалы, оно скачет по событиям. Вроде первые 15 минут фильма рассказывают о трудном детстве Нормы Джин Бейкер, а потом нам показывают её пробы в кино. Между этими событиями как бы был целый временной промежуток, где она снималась в роли фотомодели, но этому уделено секунд 15. Линия с мамой Мэрилин, которая угодила в психушку и так ни к чему и не провела. Между вторым и третьим мужем прошло 10 минут. Вместо того, чтобы показать, как Мэрилин Монро неудобно имя, которое она себе сделала, лучше бы рассказали, какой у Мэрилин был конфликт со студией 20 Век Фокс. Кстати, еще непонятно, зачем надо было посвящать отдельную линию общению Мэрилин с её типа «отцом по переписке». И что это за странная привычка называть своих мужей «папочками»?  К слову, о том, какие приоритеты расставляет режиссер. Все эти скандалы и споры Эндрю Доминика с Netflix о том, чтобы вырезать все эти откровенные сцены, яйца выеденного не стоят. Это начинает походить на игру — на какой сцене вы дропнете кино. На сцене, где Мэрилин сношает на столе кинопродюсер, на сцене, где два младших Чаплина удовлетворяют Монро во время сеанса, или где она «обслуживает» президента Кеннеди? Я — на сцене с Кеннеди, признаю, не было уже сил терпеть. Это стремление показать, как на самом деле все пользовались образом Мэрилин Монро выглядит издевательством по отношению к самой Мэрилин и Ане де Армас, которая её сыграла. И ведь актриса она хорошая, но проблема в том, что здесь её потенциал представлен лишь затравленной жизнью плаксой, которая хочет любви и понимания. Отдельным святотатством я считаю сцены, когда Мэрилин пользуются, как хотят, и на фоне играют её песни. И я не против, если нам хотят показать жестокий путь к популярности для девушки, даже если он вульгарен и пошл. Один из моих любимых фильмов (я его так, своеобразно люблю) — «Шоугелз» Пола Верховена. Не, ну а что, если я говорю о любимчиках, то это могут быть только Тарантино, Финчер и Андерсон, что ли? Если вы смотрели «Шоугелз», то понимаете, как там переходили грани дозволенного. Но та стилистика, та вакханалия, которая происходила в фильме, прекрасно отражала бум той эпохи раскрепощения и подходила Голливуду того времени. И главная героиня, хоть и бесила меня немного, но с таким трудным характером пробилась от низов танцовщиц до примы эротических шоу. Но не сказать, что создатели «Шоугелз» претендовали на серьезное кино. Я этот фильм воспринимаю как одно из развлечений, которое играет на наших животных инстинктах, аля «хлеба и зрелищ».  Возможно, перескакивать на другой фильм в контексте рецензирования этого, но «нелогично» — это именно то, как поставлена данная кинолента. «Блондинка», пытаясь почивать на лаврах серьезного авторского кино, даже номинального сюжета не имеет, потому что её пересказать сложно. Перед нами очередной долгострой, который претерпел десять лет процесса и трех претенденток на главную роль (Наоми Уоттс, Джессика Честейн и Ана де Армас). Игра не стоила свеч. Пирожок с ничем. Даже обнаженка фильм не вытащила. Вообще, я как-то спокойнее и толерантнее стал относиться к кино, и фильму надо очень постараться, чтобы пробудить меня на эмоции. Здесь я решил выплеснуть свое мнение, потому что со времен бинарных опционов я не чувствовал себя настолько обманутым.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bag_good[952])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weGLTlpLC01F",
        "outputId": "34ef7b33-b9f7-4f77-b4ca-7771bd7627fe"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "прямая ссылка\n",
            "+ комментарийдобавить комментарий\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "good_final=[]\n",
        "bad_final=[]"
      ],
      "metadata": {
        "id": "2J9_8lgkCfxK"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "while n<953:\n",
        "  good_final.append(bag_good[n])\n",
        "  n+=5"
      ],
      "metadata": {
        "id": "KPXoIGZTCQ-D"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank_of_checks=good_final[31:41]\n",
        "good_final=good_final[:31]"
      ],
      "metadata": {
        "id": "ueQqJWjeGxUi"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "while n<253:\n",
        "  bad_final.append(bag_bad[n])\n",
        "  n+=5"
      ],
      "metadata": {
        "id": "idYvGgTaDYE2"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_final=bad_final[:31]"
      ],
      "metadata": {
        "id": "bLpMKKR4HkPq"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "До этого момента мы спарсили все данные, получив в итоге два списка, в одном из которых хранятся тексты всех положительных отзывов на кино за месяц, а в другом -- отрицательных. Вся возня с n+5 была нужна, чтобы оставить именно тексты отзывов, убрав из списков никнейм автора, название фильма и тд (просто они не очень удобно парсятся на кинопоиске) + из-за этого пришлось оставить мало отзывов (31 положительный и 31 негативный)"
      ],
      "metadata": {
        "id": "BBm1XeRXDtMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь переходим к обработке текстов отзыва:"
      ],
      "metadata": {
        "id": "hU597HAoEJIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf37CvqJEUv4",
        "outputId": "f0a93ef4-4b8f-4b33-9a21-9a02a46be7c6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 3.8 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=ec6409993dc792da383278779e5b6a22c84880eabc3e41555da42359e9a43afd\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdfhBzlaJ_2o",
        "outputId": "b98de747-c417-45fb-b481-771e477b9b2f"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYs8oWgeEMKY",
        "outputId": "c0ac8cb2-15fc-4966-b65f-d211a42a1a67"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "o_sFGacvFFbM"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_good=''"
      ],
      "metadata": {
        "id": "r1XWs-UCEgfV"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for smth in good_final: \n",
        "  text_good=text_good+smth+' '"
      ],
      "metadata": {
        "id": "9BE2-bpdEcwn"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_bad=''"
      ],
      "metadata": {
        "id": "Oi6bg89UIVEY"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for smth in bad_final: \n",
        "  text_bad=text_bad+smth+' '"
      ],
      "metadata": {
        "id": "nBwyaG3AIOn1"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_good = [w.lower() for w in word_tokenize(text_good)]"
      ],
      "metadata": {
        "id": "F6ONjUc_IioS"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('russian')"
      ],
      "metadata": {
        "id": "BNgWdW-tKK9C"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_good = [w for w in tokens_good if w not in sw]"
      ],
      "metadata": {
        "id": "8jaH4EPNKeU0"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_last=[]\n",
        "import string\n",
        "punct = set(string.punctuation)\n",
        "for smth in filtered_good:\n",
        "  if smth not in punct:\n",
        "    good_last.append(smth)"
      ],
      "metadata": {
        "id": "MgLmvHkfK47H"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_lemmas = []\n",
        "for smth in good_last:\n",
        "  good_lemmas.append(morph.parse(smth)[0].normal_form)"
      ],
      "metadata": {
        "id": "dDcpRCWgLIJu"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_bad = [w.lower() for w in word_tokenize(text_bad)]"
      ],
      "metadata": {
        "id": "3LZaW-ygLkPF"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bad = [w for w in tokens_bad if w not in sw]"
      ],
      "metadata": {
        "id": "ajwRl6QyLpSx"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_last=[]\n",
        "for smth in filtered_bad:\n",
        "  if smth not in punct:\n",
        "    bad_last.append(smth)"
      ],
      "metadata": {
        "id": "pEPorJ2kLvdT"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_lemmas = []\n",
        "for smth in bad_last:\n",
        "  bad_lemmas.append(morph.parse(smth)[0].normal_form)"
      ],
      "metadata": {
        "id": "la-3XlDZL1mF"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы превратили набор плохих отзывов и набор хороших отзывов в два мешка лемм, теперь немного упорядочим их"
      ],
      "metadata": {
        "id": "0AS113qVMHn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictgood={}\n",
        "for smth in good_lemmas:\n",
        "  if smth in dictgood:\n",
        "    dictgood[smth]+=1\n",
        "  else:\n",
        "    dictgood[smth]=1"
      ],
      "metadata": {
        "id": "NoFEtkzQMSk1"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictbad={}\n",
        "for smth in bad_lemmas:\n",
        "  if smth in dictgood:\n",
        "    del dictgood[smth]\n",
        "  else:\n",
        "    if smth in dictbad:\n",
        "      dictbad[smth]+=1\n",
        "    else:\n",
        "      dictbad[smth]=1"
      ],
      "metadata": {
        "id": "-_wibfRpL7DB"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goodfiltred=[]\n",
        "badfiltred=[]\n",
        "for smth in dictgood:\n",
        "  if dictgood[smth]>2:\n",
        "    goodfiltred.append(smth)\n",
        "for smth in dictbad:\n",
        "  if dictbad[smth]>2:\n",
        "    badfiltred.append(smth)"
      ],
      "metadata": {
        "id": "G887yKtINCe-"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "таким образом, получаем два списка лемм, отфильтрованных с учетом частотности. Теперь лемматизируем оставшиеся отзывы (взятые для проверки) и создадим функцию, которая будет определять тональность отзыва."
      ],
      "metadata": {
        "id": "SAu3bK0pRcSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checks=[]\n",
        "for smth in bank_of_checks:\n",
        "  tokens_check= [w.lower() for w in word_tokenize(smth)]\n",
        "  filtered_check = [w for w in tokens_check if w not in sw]\n",
        "  last_check = [w for w in filtered_check if w not in punct]\n",
        "  lemme_check=[morph.parse(w)[0].normal_form for w in last_check]\n",
        "  checks.append(lemme_check)"
      ],
      "metadata": {
        "id": "AaCA9VVuO9Fk"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tonalizator(rate):\n",
        "  ton=0\n",
        "  for word in rate:\n",
        "    if word in goodfiltred:\n",
        "      ton+=1\n",
        "    elif word in badfiltred:\n",
        "      ton=ton-1\n",
        "  if ton>=0:\n",
        "    return(1)\n",
        "  else:\n",
        "    return(0)"
      ],
      "metadata": {
        "id": "iHFQCYu_S8Uf"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "также посчитаем accuracy"
      ],
      "metadata": {
        "id": "Ymx64Yx6UlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZPoyFDBVaqV",
        "outputId": "5c0f8e96-96db-42ef-cae8-cb87ab8e7d82"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=05b765bd927b736060c99cf1a1f1548167e886fa2e1e75a01c67b55b95c9c7e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "CtGyythpVgjd"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers=[]\n",
        "for smth in checks:\n",
        "  answers.append(tonalizator(smth))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYef1Q_VTrXY",
        "outputId": "a40a6b66-8e1e-40ae-fa76-cf26325b7e1e"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "243\n",
            "-106\n",
            "163\n",
            "-57\n",
            "457\n",
            "-161\n",
            "534\n",
            "-198\n",
            "483\n",
            "-210\n",
            "300\n",
            "-117\n",
            "162\n",
            "-66\n",
            "696\n",
            "-229\n",
            "147\n",
            "-75\n",
            "424\n",
            "-207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = accuracy_score(answers, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ],
      "metadata": {
        "id": "QOoBlDqGVki8"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUS8R9j-T2UL",
        "outputId": "58d1b242-2f62-4a90-bb84-39b347dff8cb"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score получилась ужасная -- почему? Вероятно потому, что, несмотря на то, что были взяты банки отзывов одинакового размера, в банке с уникальными словами для негативных отзывов более чем в три раза больше слов, а значит, встретить их в отзывах более вероятно. Эта проблема решается значительным увеличением количества отзывов (что мне сделать не удалось из-за неудобства парсинга), либо изменением фильтра частоты (что будет сделано в коде чуть ниже). Также хочется отметить, что одной из возможных мулек может стать векторизация отзывов (впрочем, кажется, на таком маленьком количестве данных это все равно делать не особенно есть смысл)."
      ],
      "metadata": {
        "id": "v8frhb9EVw-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goodfiltred2=[]\n",
        "badfiltred2=[]\n",
        "for smth in dictgood:\n",
        "  if dictgood[smth]>6:\n",
        "    goodfiltred2.append(smth)\n",
        "for smth in dictbad:\n",
        "  if dictbad[smth]>6:\n",
        "    badfiltred2.append(smth)"
      ],
      "metadata": {
        "id": "XFdwbah-WfVI"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tonalizator2(rate):\n",
        "  ton=0\n",
        "  for word in rate:\n",
        "    if word in goodfiltred2:\n",
        "      ton+=1\n",
        "    elif word in badfiltred2:\n",
        "      ton=ton-1\n",
        "  if ton>=0:\n",
        "    return(1)\n",
        "  else:\n",
        "    return(0)"
      ],
      "metadata": {
        "id": "pXuY2Jv-XKyy"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers2=[]\n",
        "for smth in checks:\n",
        "  answers2.append(tonalizator2(smth))"
      ],
      "metadata": {
        "id": "L-iLFrYHXQ0-"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "это не помогает, тк негативных слов все еще в 4-5 раз больше. Может помочь следующее решение: отфильтровать списки слов и от каждого оставить n самых частотных"
      ],
      "metadata": {
        "id": "WOfMNVkUXii5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_good = {}\n",
        "sorted_keys = sorted(dictgood, key=dictgood.get, reverse=True) \n",
        "\n",
        "for w in sorted_keys:\n",
        "    sorted_good[w] = dictgood[w]\n",
        "\n",
        "top200good=list(sorted_good.keys())[1:201]"
      ],
      "metadata": {
        "id": "IlpFD_uEXuWg"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_bad = {}\n",
        "sorted_keys = sorted(dictbad, key=dictbad.get, reverse=True) \n",
        "\n",
        "for w in sorted_keys:\n",
        "    sorted_bad[w] = dictbad[w]\n",
        "\n",
        "top200bad=list(sorted_bad.keys())[1:201]"
      ],
      "metadata": {
        "id": "WIcqNSZdZXW1"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tonalizator3(rate):\n",
        "  ton=0\n",
        "  for word in rate:\n",
        "    if word in top200good:\n",
        "      ton+=1\n",
        "    elif word in top200bad:\n",
        "      ton=ton-1\n",
        "  if ton>=0:\n",
        "    return(1)\n",
        "  else:\n",
        "    return(0)"
      ],
      "metadata": {
        "id": "Ok-iHxFEZgdi"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers=[]\n",
        "for smth in checks:\n",
        "  answers.append(tonalizator(smth))"
      ],
      "metadata": {
        "id": "nO1OqRsMZy6b"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "все еще не работает, как должно, и все еще виню в этом нехватку отзывов. Соответственно, и какуюзнибудь векторизацию (которую в теории можно предложить как мульку, улучшающую качество), тоже сейчас использовать смысла нет."
      ],
      "metadata": {
        "id": "eT_7l3qHaArl"
      }
    }
  ]
}